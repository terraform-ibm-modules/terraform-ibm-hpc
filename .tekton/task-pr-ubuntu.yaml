---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: wes-hpc-da-ubuntu-pr
spec:
  params:
    - name: ibmcloud-api
      description: the ibmcloud api
      default: https://cloud.ibm.com
    - name: continuous-delivery-context-secret
      description: name of the secret containing the continuous delivery pipeline context secrets
      default: secure-properties
    - name: ibmcloud-apikey-secret-key
      description: field in the secret that contains the api key used to login to ibmcloud
      default: ibmcloud_api_key
    - name: pipeline-debug
      description: Pipeline debug mode. Value can be 0 or 1. Default to 0
      default: "0"
    - name: pr-branch
      description: The source branch for the PullRequest
      default: ""
    - name: directory-name
      default: "."
    - name: repository
      description: the git repo url
    - name: ssh_keys
      default: ""
      description: List of names of the SSH keys that is configured in your IBM Cloud account, used to establish a connection to the IBM Cloud HPC bastion and login node. Ensure that the SSH key is present in the same resource group and region where the cluster is being provisioned. If you do not have an SSH key in your IBM Cloud account, create one by according to [SSH Keys](https://cloud.ibm.com/docs/vpc?topic=vpc-ssh-keys).
    - name: zones
      default: ""
      description: IBM Cloud zone names within the selected region where the IBM Cloud HPC cluster should be deployed. Two zone names are required as input value and supported zones for eu-de are eu-de-2, eu-de-3 and for us-east us-east-1, us-east-3. The management nodes and file storage shares will be deployed to the first zone in the list. Compute nodes will be deployed across both first and second zones, where the first zone in the list will be considered as the most preferred zone for compute nodes deployment. [Learn more](https://cloud.ibm.com/docs/vpc?topic=vpc-creating-a-vpc-in-a-different-region#get-zones-using-the-cli).
    - name: cluster_prefix
      description: Prefix that is used to name the IBM Cloud HPC cluster and IBM Cloud resources that are provisioned to build the IBM Cloud HPC cluster instance. You cannot create more than one instance of the IBM Cloud HPC cluster with the same name. Ensure that the name is unique.
      default: cicd-wes
    - name: resource_group
      description: Resource group name from your IBM Cloud account where the VPC resources should be deployed. Note. If the resource group value is set as null, automation creates two different RG with the name (workload-rg and service-rg). For additional information on resource groups, see [Managing resource groups](https://cloud.ibm.com/docs/account?topic=account-rgs).
      default: Default
    - name: remote_allowed_ips
      default: ""
      description: Comma-separated list of IP addresses that can access the IBM Cloud HPC cluster instance through an SSH interface. For security purposes, provide the public IP addresses assigned to the devices that are authorized to establish SSH connections (for example, [\"169.45.117.34\"]). To fetch the IP address of the device, use [https://ipv4.icanhazip.com/](https://ipv4.icanhazip.com/).
    - name: compute_image_name_rhel
      description: Name of the custom image that you want to use to create virtual server instances in your IBM Cloud account to deploy the IBM Cloud HPC cluster dynamic compute nodes. By default, the solution uses a RHEL 8-6 OS image with additional software packages mentioned [here](https://cloud.ibm.com/docs/hpc-spectrum-LSF#create-custom-image). The solution also offers, Ubuntu 22-04 OS base image (hpcaas-lsf10-ubuntu2204-compute-v1). If you would like to include your application-specific binary files, follow the instructions in [ Planning for custom images ](https://cloud.ibm.com/docs/vpc?topic=vpc-planning-custom-images) to create your own custom image and use that to build the IBM Cloud HPC cluster through this offering.
      default: ""
    - name: compute_image_name_ubuntu
      description: Name of the custom image that you want to use to create virtual server instances in your IBM Cloud account to deploy the IBM Cloud HPC cluster dynamic compute nodes. By default, the solution uses a RHEL 8-6 OS image with additional software packages mentioned [here](https://cloud.ibm.com/docs/hpc-spectrum-LSF#create-custom-image). The solution also offers, Ubuntu 22-04 OS base image (hpcaas-lsf10-ubuntu2204-compute-v1). If you would like to include your application-specific binary files, follow the instructions in [ Planning for custom images ](https://cloud.ibm.com/docs/vpc?topic=vpc-planning-custom-images) to create your own custom image and use that to build the IBM Cloud HPC cluster through this offering.
      default: ""
    - name: login_image_name
      description: Name of the custom image that you want to use to create virtual server instances in your IBM Cloud account to deploy the IBM Cloud HPC cluster login node. By default, the solution uses a RHEL 8-6 OS image with additional software packages mentioned [here](https://cloud.ibm.com/docs/hpc-spectrum-LSF#create-custom-image). The solution also offers, Ubuntu 22-04 OS base image (hpcaas-lsf10-ubuntu2204-compute-v2). If you would like to include your application-specific binary files, follow the instructions in [ Planning for custom images ](https://cloud.ibm.com/docs/vpc?topic=vpc-planning-custom-images) to create your own custom image and use that to build the IBM Cloud HPC cluster through this offering.
      default: ""
    - name: cluster_id
      description: Ensure that you have received the cluster ID from IBM technical sales. A unique identifer for HPC cluster used by IBM Cloud HPC to differentiate different HPC clusters within the same reservation. This can be up to 39 alphanumeric characters including the underscore (_), the hyphen (-), and the period (.) characters. You cannot change the cluster ID after deployment.
      default: ""
    - name: reservation_id
      description: Ensure that you have received the reservation ID from IBM technical sales. Reservation ID is a unique identifier to distinguish different IBM Cloud HPC service agreements. It must start with a letter and can only contain letters, numbers, hyphens (-), or underscores (_).
      default: ""
  workspaces:
    - name: workspace
      mountPath: /artifacts
  stepTemplate:
    env:
      - name: API_KEY
        valueFrom:
          secretKeyRef:
            name: $(params.continuous-delivery-context-secret)
            key: $(params.ibmcloud-apikey-secret-key)
            optional: true
      - name: PIPELINE_DEBUG
        value: $(params.pipeline-debug)
  steps:
    - name: test-run-basic-ubuntu-pr
      onError: continue
      image: icr.io/continuous-delivery/pipeline/pipeline-base-ubi:latest
      env:
        - name: ssh_keys
          value: $(params.ssh_keys)
        - name: zones
          value: $(params.zones)
        - name: resource_group
          value: $(params.resource_group)
        - name: compute_image_name_ubuntu
          value: $(params.compute_image_name_ubuntu)
        - name: login_image_name
          value: $(params.login_image_name)
        - name: cluster_id
          value: $(params.cluster_id)
        - name: reservation_id
          value: $(params.reservation_id)
      workingDir: "/artifacts"
      imagePullPolicy: Always
      command: ["/bin/bash", "-c"]
      args:
        - |
          #!/bin/bash

          if [[ "${PIPELINE_DEBUG}" == "true" ]]; then
            pwd
            env
            trap env EXIT
            set -x
          fi

          unzip -n terraform_1.5.7_linux_amd64.zip
          mv terraform /usr/local/bin/terraform
          terraform --version

          cd $(pwd)/ &&
          echo "export PATH=\$PATH:$(pwd)/go/bin:\$HOME/go/bin" >> ~/.bashrc &&
          echo "export GOROOT=$(pwd)/go" >> ~/.bashrc
          source ~/.bashrc
          go version

          export TF_VAR_ibmcloud_api_key=$API_KEY
          export SSH_FILE_PATH="/artifacts/.ssh/id_rsa"
          CICD_SSH_KEY="cicd-toolchain"

          # Check artifacts/tests folder exists or not
          DIRECTORY="/artifacts/tests"
          if [ -d "$DIRECTORY" ]; then
            cd $DIRECTORY
              LOG_FILE=pipeline-TestRunBasic-ubuntu-$(date +%d%m%Y).cicd
              echo "**************Validating on TestRunBasic**************"
              SSH_KEY=$CICD_SSH_KEY COMPUTE_IMAGE_NAME=$compute_image_name_ubuntu LOGIN_NODE_IMAGE_NAME=$login_image_name ZONE=$zones RESERVATION_ID=$reservation_id CLUSTER_ID=$cluster_id RESOURCE_GROUP=$resource_group go test -v -timeout 9000m -run TestRunBasic | tee -a $LOG_FILE
          else
            pwd
            ls -a
            echo "$DIRECTORY does not exists"
            exit 1
          fi

          # Check any error message on the plan/apply log
          error_check=$(eval "grep -E -w 'FAIL|Error|ERROR' $DIRECTORY/$LOG_FILE")
          if [[ "$error_check" ]]; then
            echo "Found Error/FAIL/ERROR in plan/apply log. Please check log."
            exit 1
          fi

          echo "*******************************************************"
          count=`ls -1 $DIRECTORY/test_output/log* 2>/dev/null | wc -l`
          if [ $count == 0 ]; then
            echo "Test Suite have not initated and log file not created, check with packages or binaries installation"
            exit 1
          else
            cat $DIRECTORY/test_output/log*
          fi
          echo "*******************************************************"

          count=`ls -1 $DIRECTORY/*.cicd 2>/dev/null | wc -l`
          if [ $count == 0 ]; then
            echo "Test Suite have not initated, check with packages or binaries installation"
            exit 1
          fi
